{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da7b9b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mood': 'Angry', 'recommendations': ['Boston - Hitch a Ride', 'Red Hot Chili Peppers - Nobody Weird Like Me - Remastered', 'Arctic Monkeys - The View From The Afternoon', \"Buffalo Springfield - For What It's Worth\", 'Velvet Two Stripes - Drinks', 'The Stone Roses - Driving South', 'Better Than Ezra - Good', 'Silversun Pickups - Lazy Eye', 'U2 - The Blackout', 'The Clash - The Magnificent Seven - Remastered', 'The Last Shadow Puppets - Bad Habits', 'Sly & The Family Stone - I Want To Take You Higher - Live at The Woodstock Music & Art Fair, August 17, 1969', 'Berry Sakharof - ‚óä‚Ä†‚óä¬ß‚óä‚Ñ¢‚óä√∫‚óä√¥ ‚óä√Æ‚óä√¨‚óä√≠', 'Soen - Lotus', 'Liam Gallagher - Once', 'El Goodo - Feel so Fine', 'Why Bonnie - Athlete', 'MOD SUN - Stay Away (feat. Machine Gun Kelly & Goody Grace)', 'The Black Crowes - Sometimes Salvation', 'Radiohead - Burn the Witch', 'Arctic Monkeys - R U Mine?', 'The Stranglers - Golden Brown', 'Joe Bonamassa - Cradle Rock', \"Primal Scream - Movin' on Up\", 'Sharon Van Etten - Comeback Kid', 'beabadoobee - Don‚Äö√Ñ√¥t You (Forget About Me) - live from Abbey Road Studios, London', 'R.E.M. - Drive', 'ANSON - Ghost', 'Jimmy Eat World - All The Way (Stay)', 'Ruthi Navon - ‚óä√≥‚óä¬©‚óä√ª‚óä√∫ ‚óä√´‚óä√µ‚óä¬ß‚óä√Ø‚óä‚Ñ¢ ‚óä√¥‚óä√¨‚óä√¥‚óä√∂', 'Violent Soho - Covered in Chrome', 'Cage The Elephant - Skin and Bones', 'Faith No More - Falling to Pieces', 'The Offspring - Coming for You', 'Pain of Salvation - Meaningless', 'Lenny Kravitz - Stand By My Woman', 'Black Sabbath - Paranoid', 'The First Edition - Just Dropped In (To See What Condition My Condition Was In)', 'Bryan Adams - I Think About You', \"The 1975 - She's American\", 'Piers Faccini - Bring Down The Wall', 'Thirty Seconds To Mars - Dangerous Night', 'Bring Me The Horizon - Parasite Eve', 'Jimi Hendrix - Purple Haze', 'Eskiz - Aborjinin T‚àö¬∫rk‚àö¬∫s‚àö¬∫', 'Neon Trees - Animal', 'Scary Kids Scaring Kids - Holding On', \"Berry Sakharof - 77' - Acoustic Live\", 'Breezie 311 - Shots In The Dark', 'Arcade Fire - Ready to Start']}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://localhost:5000/chat\"\n",
    "payload = {\"text\": \"I feel down and exhausted lately.\"}\n",
    "\n",
    "response = requests.post(url, json=payload)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51d8ded6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fluidsynth' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!fluidsynth --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cd97ce3",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xe0 in position 0: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Load model and feature extractor\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m extractor \u001b[38;5;241m=\u001b[39m \u001b[43mAutoFeatureExtractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForAudioClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(MODEL_PATH)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     20\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Users\\parth\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\auto\\feature_extraction_auto.py:341\u001b[0m, in \u001b[0;36mAutoFeatureExtractor.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    338\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    339\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_auto\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 341\u001b[0m config_dict, _ \u001b[38;5;241m=\u001b[39m FeatureExtractionMixin\u001b[38;5;241m.\u001b[39mget_feature_extractor_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    342\u001b[0m feature_extractor_class \u001b[38;5;241m=\u001b[39m config_dict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_extractor_type\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    343\u001b[0m feature_extractor_auto_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\parth\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\feature_extraction_utils.py:527\u001b[0m, in \u001b[0;36mFeatureExtractionMixin.get_feature_extractor_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    524\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    525\u001b[0m     \u001b[38;5;66;03m# Load feature_extractor dict\u001b[39;00m\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(resolved_feature_extractor_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m reader:\n\u001b[1;32m--> 527\u001b[0m         text \u001b[38;5;241m=\u001b[39m \u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    528\u001b[0m     feature_extractor_dict \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(text)\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m json\u001b[38;5;241m.\u001b[39mJSONDecodeError:\n",
      "File \u001b[1;32mc:\\Users\\parth\\AppData\\Local\\Programs\\Python\\Python310\\lib\\codecs.py:322\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[0;32m    321\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m--> 322\u001b[0m     (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;66;03m# keep undecoded input until the next call\u001b[39;00m\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m=\u001b[39m data[consumed:]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xe0 in position 0: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import librosa\n",
    "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification\n",
    "import argparse\n",
    "\n",
    "# -------------------------------\n",
    "# CONFIGURATION\n",
    "# -------------------------------\n",
    "\n",
    "# Replace with your actual model directory or HF hub ID\n",
    "MODEL_PATH = \"models/genre_classification/model.safetensors\"  # e.g., \"./genre_model\" or \"username/model-id\"\n",
    "\n",
    "# Set computation device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load model and feature extractor\n",
    "extractor = AutoFeatureExtractor.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForAudioClassification.from_pretrained(MODEL_PATH).to(device)\n",
    "model.eval()\n",
    "\n",
    "# -------------------------------\n",
    "# AUDIO PREPROCESSING FUNCTION\n",
    "# -------------------------------\n",
    "\n",
    "def preprocess_audio(audio_path, sampling_rate=16000):\n",
    "    audio, sr = librosa.load(audio_path, sr=sampling_rate)\n",
    "    return {\"array\": audio, \"sampling_rate\": sr}\n",
    "\n",
    "# -------------------------------\n",
    "# PREDICTION FUNCTION\n",
    "# -------------------------------\n",
    "\n",
    "def predict_genre(audio_path):\n",
    "    audio_input = preprocess_audio(audio_path)\n",
    "    features = extractor(audio_input[\"array\"], sampling_rate=audio_input[\"sampling_rate\"], return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = {k: v.to(device) for k, v in features.items()}\n",
    "        logits = model(**inputs).logits\n",
    "        predicted_class_id = torch.argmax(logits, dim=1).item()\n",
    "        probabilities = F.softmax(logits, dim=1).cpu().numpy().squeeze()\n",
    "\n",
    "    label = model.config.id2label[predicted_class_id]\n",
    "    return label, probabilities\n",
    "\n",
    "# -------------------------------\n",
    "# CLI INTERFACE\n",
    "# -------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Genre classification from audio file\")\n",
    "    parser.add_argument(\"audio_path\", type=str, help=\"Path to the .wav audio file\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    genre, probs = predict_genre(args.audio_path)\n",
    "\n",
    "    print(f\"\\nüé∂ Predicted Genre: {genre}\")\n",
    "    print(\"üéõÔ∏è  Class Probabilities:\")\n",
    "    for i, (label_id, label_name) in enumerate(model.config.id2label.items()):\n",
    "        print(f\"  {label_name}: {probs[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efa82d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0501 10:38:16.514000 38304 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model and extractor saved to: models/genre_model_clean\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForAudioClassification, AutoFeatureExtractor\n",
    "\n",
    "# Path to your extracted folder (where model.safetensors and config.json exist)\n",
    "MODEL_PATH = \"models/genre_classification\"\n",
    "SAVE_PATH = \"models/genre_model_clean\"\n",
    "\n",
    "# Load and re-save model and extractor\n",
    "model = AutoModelForAudioClassification.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "extractor = AutoFeatureExtractor.from_pretrained(MODEL_PATH)\n",
    "\n",
    "model.save_pretrained(SAVE_PATH, safe_serialization=True)\n",
    "extractor.save_pretrained(SAVE_PATH)\n",
    "\n",
    "print(f\"‚úÖ Model and extractor saved to: {SAVE_PATH}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
